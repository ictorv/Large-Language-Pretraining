## Self Attention with Trainable Weights
or `Scaled dot product Attention`

Previously we have done:
1. Compute Attention Scores (Dot Product)
2. Compute Attention Weights (Normalization - Applied Softmax)
3. Compute Context Vectors (Multiply Each Attention Score with Weights)

## So Need Trainable Weights
1. Introduce `weight matrices` that update during model

> Three trainable weights matrics:  
    -- Query     
    -- Key   
    -- Value    
<img src="assets/15. Self Attention/multipl.png" width="500" />    

2. Compute Attention Score (Dot product between $Query$ with $Key^T$ )
> But it is not interpretable, thus need Normalization will also help in Backpropagation 

> **INTERVIEW QN**: Why Normalization of Attention Score Needed


3. Compute Attention Weights (Normalised)
But Before Normalization we should scale by $\sqrt{dim(keys)}$.  
Then Apply Softmax(Normalization)

> **INTERVIEW QN**: Why Scaling of Score with $\sqrt{dim(keys)}$ require?

> Reason 1: If have applies softmax in bigger number then we get bigger number and model will say with confidence yes! That is next word or That is only relatable to me!

<img src="assets/15. Self Attention/only.gif" width="300" />    

> Reason 2: When we multiply Query and $Key^T$ resultant matrix's **Variance** also increases proportionally .  

<img src="assets/15. Self Attention/var.gif" width="300" />    

Using **Square root** makes variance close to one

Therefore it is called **Scaled Dot Product Vector**

4. Find Context Vector: Multiply Attention Weight with corresponding Values then sum them up.

## Overall 
```python
import torch.nn as nn
class SelfAttention(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        # Step 1
        self.d_out = d_out
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))
    
    def forward(self, x):
        # Step 2
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        # Step 3
        attn_scores = queries @ keys.T 
        attn_weights = torch.softmax(
                            attn_scores / keys.shape[-1]**0.5, dim=-1
                        )
        # Step 4
        context_vec = attn_weights @ values
        return context_vec
```

### Improvements: Can use nn.Linear Initialisation which is better for model training

```python
class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        # nn.Linear
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)
        context_vec = attn_weights @ values
        return context_vec
```
### Conclusion
<img src="assets/15. Self Attention/selfatt.png" width="500" />    

**Query**: Analogous to search query in a database. It Represent the current token model.

**Key**: In Attention Mechanism each item in input sequence has a key. Keys are used to match with the query.

**Value**: It represent actual content or representation of input items once model determine which keys (which part of the input) are most relevant to the query (current focus item), it retrives the corresponding values.