# Positional Encoding
- Built in top of Token Encoding/Embedding.

## Why Positional Encoding Needed
>The `cat` sat on the mat  
On the mat the `cat` sat

Same vector for `cat` in different sentences.

<img src="assets/11. Positional Encoding/pos.png" width="500" />   

- It is helpful to inject additional position information to the LLM.
- There are two types of positional embeddings.

A. `Absolute`  
Absolute position is added
<img src="assets/11. Positional Encoding/abs.png" width="500" />   

Positional vectors have same dimension as the original token embeddings.

B. `Relative`
Emphasis on relative position or distance between tokens.

The model learn the relationship in terms of "how far apart" rather than at which exact position.

**Advantage**: Model can generalise better to sequence fo `varying length`, even if it has not seen such length during training.

Both type of positional encoding better then not using any type of positional encoding. It enable LLMs to `understand the order and relationship` between tokens.

Absolute: When fixed order of token is crucial, eg: sequence generation. 
- GPT was trained on this and original Transformer too.
- Used more commonly.

Relative: Suitable for language modelling over long sequence, where same phrase xan repeat over and over again.

#### Positional embedding also need to be optimised during training process. This optimization is part of model training itself.

