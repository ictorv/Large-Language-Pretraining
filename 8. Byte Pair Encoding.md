# Byte Pair Encoding
- Much more sophisticated tokenization scheme.
- Was used to train modern LLM.

## Tokenization Algorithm
1. Word Based "My hobby is playing cricket" -> ['My' , 'hobby', 'is', 'playing', 'cricket']
Problem: 
- What to do with Out of Vocabulary (OOV) words.
- boy & boys are similar bur similarity is not capture

2. **`Sub-word Based`**
- It capture root words(Boy & Boys)  
There are 2 Rules: 

A. Do `not` split `frequently` used words into smaller subwords. [Feature from word level]  

B. `Split` the `rare words` into smaller, meaning subwords. [Feature from character level]

Features:
- Subword splitting hepls model learn differnet word with same root word as "token" like "tokens" and "tokenizing" are `similar` in meaning.
- It also helps the model learn that `"tokenization"` and `"modernization"` are made up of differnet root words but have the same suffix `"ization"` and are used in same syntatic situations.

3. Character Based
"My hobby is playing cricket" -> ['M','y','H','o',.....]

Problem:
- Small Vocabulary (every language have fixed character)
- Meaning associated word is completely lost.
- Tokenized sequence is much longer than initial raw text. 

## BPT
Byte Pair Encoding is subword tokenization algorithm

History:
- Introduced in 1994, was data compression algorithm. [Paper](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)
<img src="assets/8. BPE/paper.png" width="500" />    

- `Most common` pair of `consecutive bytes` of data is `replaced` with a byte that does not occur in data.

## Practical Demonstration of BPE
Original Data: aaabdaaabac

- Most occurance: `'aa'`: 4 times
- Replace it with Z  
  Compressed data: **Z**abd**Z**abac

- Next Common Byte pair: `'ab'`
- Replace it with Y  
  Compressed data: Z**Y**dZ**Y**ac
Only `ac` byte pair left but appear only once.

- We can also compress `ZY` replace it by `W`  
  Compressed data: **W**d**W**ac

## What to do BPE with it with LLM?
Rule I: BPE ensures that most word in the vocabulary are represented as a `single token`, while `rare words` are `broken` down into two or more subword token.

## Practical Example
- Let's consider below dataset of words:
{"old":7,"older:3,"finest":9,"lowest":4}

- Preprocessing : We need to add end token `</w>` at the end of each word.
{`"old</w>"`:7,`"older`</w>:3,`"finest</w>"`:9,`"lowest</w>"`:4}

- Split word into character and count their frequency.

| Number | Token | Frequency |
|--------|-------|-----------|
| 1      | `</w>`  | 23        |
| 2      | o     | 14        |
| 3      | l     | 14        |
| 4      | d     | 10        |
| 5      | e     | 16        |
| 6      | r     | 3         |
| 7      | f     | 9         |
| 8      | i     | 9         |
| 9      | n     | 9         |
| 10     | s     | 13        |
| 11     | t     | 13        |
| 12     | w     | 4         |

- Look for the most frequent pairing .
Merge them and perform the iteration again and again until we reach the token limit or iteration limit.

Iteration 1: Most appeared `e` pairing with `s`

| Number | Token | Frequency  |
|--------|-------|------------|
|   1    | `</w>`  | 23         |
|   2    | o     | 14         |
|   3    | l     | 14         |
|   4    | d     | 10         |
|   5    | e     | 16 - 13 = 3 |
|   6    | r     | 3          |
|   7    | f     | 9          |
|   8    | i     | 9          |
|   9    | n     | 9          |
|  10    | s     | 13 - 13 = 0 |
|  11    | t     | 13         |
|  12    | w     | 4          |
|  13    | es    | 9 + 4 = 13  |

Iteration 2: Merge the token `es` and `t` as they appeared 13 times in our dataset

| Number | Token | Frequency        |
|--------|-------|-----------------|
|   1    | `</w>`  | 23              |
|   2    | o     | 14              |
|   3    | l     | 14              |
|   4    | d     | 10              |
|   5    | e     | 16 - 13 = 3      |
|   6    | r     | 3               |
|   7    | f     | 9               |
|   8    | i     | 9               |
|   9    | n     | 9               |
|  10    | s     | 13 - 13 = 0      |
|  11    | t     | 13 - 13 = 0      |
|  12    | w     | 4               |
|  13    | es    | 9 + 4 = 13 - 13 = 0 |
|  14    | est   | 13              |

Now look into `est` with `</w>`

| Number | Token   | Frequency           |
|--------|---------|---------------------|
| 1      | `<w>`   | 23 - 13 = 10        |
| 2      | o       | 14                  |
| 3      | l       | 14                  |
| 4      | d       | 10                  |
| 5      | e       | 16 - 13 = 3         |
| 6      | r       | 3                   |
| 7      | i       | 9                   |
| 8      | n       | 9                   |
| 9      | s       | 13 - 13 = 0         |
| 10     | t       | 13 - 13 = 0         |
| 11     | w       | 4                   |
| 12     | es      | 9 + 4 - 13 = 0      |
| 13     | est`<w>`  | 13                  |

`est<w>` to differentiate and know that it is ending word. 

Iteration 3: Merge `est` with `</w>`

| Number | Token     | Frequency          |
|--------|-----------|--------------------|
| 1      | `</w>`    | 10 (23 - 13 = 10)  |
| 2      | `o`       | 14                 |
| 3      | `l`       | 14                 |
| 4      | `d`       | 10                 |
| 5      | `e`       | 3 (16 - 13 = 3)    |
| 6      | `r`       | 3                  |
| 7      | `f`       | 9                  |
| 8      | `i`       | 9                  |
| 9      | `n`       | 9                  |
| 10     | `s`       | 0 (13 - 13 = 0)    |
| 11     | `t`       | 0 (13 - 13 = 0)    |
| 12     | `w`       | 4                  |
| 13     | `es`      | 0 (9 + 4 - 13 = 0) |
| 14     | `est`     | 0 (13 - 13 = 0)    |
| 15     | `est</w>` | 13                 |

Iteration 4: `o` and `l` to be combined

| Number | Token     | Frequency          |
|--------|-----------|--------------------|
| 1      | `</w>`    | 23                 |
| 2      | `o`       | 4 (14 - 10 = 4)    |
| 3      | `l`       | 4 (14 - 10 = 4)    |
| 4      | `d`       | 10                 |
| 5      | `e`       | 3 (16 - 13 = 3)    |
| 6      | `r`       | 3                  |
| 7      | `f`       | 9                  |
| 8      | `i`       | 9                  |
| 9      | `n`       | 9                  |
| 10     | `s`       | 0 (13 - 13 = 0)    |
| 11     | `t`       | 0 (13 - 13 = 0)    |
| 12     | `w`       | 4                  |
| 13     | `es`      | 0 (9 + 4 - 13 = 0) |
| 14     | `est`     | 13                 |
| 15     | `ol`      | 10 (7 + 3 = 10)    |

Iteration 5: `ol` combine with `d`

| Number | Token     | Frequency                  |
|--------|-----------|----------------------------|
| 1      | `</w>`    | 10 (23 - 13 = 10)          |
| 2      | `o`       | 4 (14 - 10 = 4)            |
| 3      | `l`       | 4 (14 - 10 = 4)            |
| 4      | `d`       | 0 (10 - 10 = 0)            |
| 5      | `e`       | 3 (16 - 13 = 3)            |
| 6      | `r`       | 3                          |
| 7      | `f`       | 9                          |
| 8      | `i`       | 9                          |
| 9      | `n`       | 9                          |
| 10     | `s`       | 0 (13 - 13 = 0)            |
| 11     | `t`       | 0 (13 - 13 = 0)            |
| 12     | `w`       | 4                          |
| 13     | `es`      | 0 (9 + 4 - 13 - 13 = 0)    |
| 14     | `est`     | 13                         |
| 15     | `est</w>` | 13                         |
| 16     | `ol`      | 0 (7 + 3 - 10 - 10 = 0)    |
| 17     | `old`     | 10 (7 + 3 = 10)            |

`f`, `i` and `n` appear 9 times but we have just one word with these characters. so, we are not merging.

Let's remove token with frequency with 0

| Number | Token  | Frequency |
|--------|--------|-----------|
| 1      | `</w>`   | 10        |
| 2      | o      | 4         |
| 3      | l      | 4         |
| 4      | e      | 3         |
| 5      | r      | 3         |
| 6      | f      | 9         |
| 7      | i      | 9         |
| 8      | n      | 9         |
| 9      | w      | 4         |
| 10     | est`</w>`| 13        |
| 11     | old    | 10        |

`Observation`: Captured `root word` and `prefix`

Stopping Criteria while encoding:
- Token count
- Number of iteration

When GPT 2 were trained there were only 50,257 tokens due to BPE. 
It also solves out of vocabulary problem