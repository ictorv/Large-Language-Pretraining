# Byte Pair Encoding
- Much more sophisticated tokenization scheme.
- Was used to train modern LLM.

## Tokenization Algorithm
1. Word Based "My hobby is playing cricket" -> ['My' , 'hobby', 'is', 'playing', 'cricket']
Problem: 
- What to do with Out of Vocabulary (OOV) words.
- boy & boys are similar bur similarity is not capture

2. **`Sub-word Based`**
- It capture root words(Boy & Boys)  
There are 2 Rules: 

A. Do `not` split `frequently` used words into smaller subwords. [Feature from word level]  

B. `Split` the `rare words` into smaller, meaning subwords. [Feature from character level]

Features:
- Subword splitting hepls model learn differnet word with same root word as "token" like "tokens" and "tokenizing" are `similar` in meaning.
- It also helps the model learn that `"tokenization"` and `"modernization"` are made up of differnet root words but have the same suffix `"ization"` and are used in same syntatic situations.

3. Character Based
"My hobby is playing cricket" -> ['M','y','H','o',.....]

Problem:
- Small Vocabulary (every language have fixed character)
- Meaning associated word is completely lost.
- Tokenized sequence is much longer than initial raw text. 

## BPT
Byte Pair Encoding is subword tokenization algorithm

History:
- Introduced in 1994, was data compression algorithm. [Paper](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)
<img src="assets/7. BPE/paper.png" width="500" />    

- `Most common` pair of `consecutive bytes` of data is `replaced` with a byte that does not occur in data.

## Practical Demonstration of BPE
Original Data: aaabdaaabac

- Most occurance: `'aa'`: 4 times
- Replace it with Z  
  Compressed data: **Z**abd**Z**abac

- Next Common Byte pair: `'ab'`
- Replace it with Y  
  Compressed data: Z**Y**dZ**Y**ac
Only `ac` byte pair left but appear only once.

- We can also compress `ZY` replace it by `W`  
  Compressed data: **W**d**W**ac

## What to do BPE with it with LLM?
Rule I: BPE ensures that most word in the vocabulary are represented as a `single token`, while `rare words` are `broken` down into two or more subword token.

## Practical Example
