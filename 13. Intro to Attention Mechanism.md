# Intro to Attention Mechanism

## Why Attention Mechanism?
> "The `cat` that was `sitting` on the mat,  
which was next to the dog, `jumped.`"

There are some words related to `cat` to which it LLM should pay attention in order to understand about `cat`

<img src="assets/13. Intro Attention/jump.gif" width="500" />    

Neural Network doesn't capture Long term dependencies

## 4 Type if Attention Mechanism
**A.** Simplified Self Attention

**B.** Self Attention:
Trainable weights

**C.** Casual Attention: 
Consider previous and current inputs

**D.** Multi-Head Attention: 
Enable to simultaneously attend to information from different reprsentation subspace.

## History
### Problem withh modelling long sequence
<img src="assets/13. Intro Attention/trans.png" width="500" />    

`
Eg: Harry Potter went to station number 93 by 4 ....
`  
We need `memory` yo know station number

To address this issue Encoder and Decoder was introduced

`Encoder`: Receive and Read german text and pass to decoder
`Decoder`: Translate to English Text

<img src="assets/13. Intro Attention/im.gif" width="500" />    

`Fig :Encoder get german language and converts to **context vector** which contains meaning then sends to decoder. Decoder generate output`
  
  
  
>Before Transformer, **RNN** were most popular encoder-decoder architecture for language translation.
<img src="assets/13. Intro Attention/endec.png" width="500" />    
