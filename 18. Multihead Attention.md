# Implementing multihead attention with weight split

GPT contains 96 multihead attention so $96 \times 3$ matrix will be needed which can be reduced

Previously we have done:

![Previous Diagram](assets/17.%20Multihead%20Attention/prev.png)

#### $head\_dimension = \frac{d_{out}}{n_{head}}$

## Step 1:  Input X consist of 3 things 
- $b$: batch
- $num_{tokens}$: number of token
- $d_{in}$: input dimension 

$$\mathbf{X}=\begin{bmatrix} 
1.0 & 2.0 & 3.0 & 4.0 & 5.0 & 6.0 \\ 
6.0 & 5.0 & 4.0 & 3.0 & 2.0 & 1.0 \\ 
1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 
\end{bmatrix}$$

- Consider 3 words with 6 dimension vector

## Step 2: Decide $d_{out}$ and $num_{heads}$

$d_{out}=6$
$num_{heads}=2$

$head\_dimension = \frac{d_{out}}{n_{head}} = \frac{6}{2} =3$

## Step 3: Initial Trainable weight for key, query, value ($W_k$, $W_q$, $W_v$)

- Since $d_in, d_out =(6*6)$ we need to make it compatible

Here is Random Initialisation, used Linear Layer

$$\mathbf{W_q} = \begin{bmatrix} 
0.6323 & -0.2366 & 1.2455 & 0.3465 & 1.2458 & 0.3229 \\ 
0.6571 & -0.2378 & -0.5311 & -0.2610 & -1.4819 & -1.6418 \\ 
-0.2990 & 0.4216 & 0.2114 & -0.0271 & -0.5682 & 0.6937 \\ 
-1.1291 & -1.0102 & 0.6946 & 0.1094 & 0.5130 & -0.8669 \\ 
0.3480 & 0.2593 & 0.4412 & 1.0017 & -0.3913 & -0.2878 \\ 
0.2484 & 0.2846 & -0.3386 & -0.6164 & 1.2722 & 0.5754 
\end{bmatrix}$$

$$\mathbf{W_k} = \begin{bmatrix} 
-0.3703 & 0.5431 & -0.0372 & -0.4406 & 0.4103 & -0.1773 \\ 
1.5993 & -0.2777 & -1.1989 & -0.4301 & 0.6927 & -1.3384 \\ 
1.2478 & -0.1872 & -8.1678 & 1.4382 & 1.2927 & 0.4822 \\ 
-0.8984 & -0.8983 & 8.3334 & -0.6312 & 0.1022 & -1.0715 \\ 
-0.7647 & 0.1734 & 0.6305 & 1.0155 & 0.8474 & 0.1454 \\ 
-1.5085 & -0.4529 & 0.0997 & -0.1084 & 0.8046 & 0.3459 
\end{bmatrix}$$

$$\mathbf{W_v} = \begin{bmatrix} 
1.6395 & 1.1234 & -0.1001 & 0.5021 & -1.0590 & 0.1412 \\ 
-8.4271 & 0.5681 & 0.4164 & -1.2534 & 1.3061 & 0.3610 \\ 
-0.2824 & -0.4314 & 1.2358 & 0.1181 & -1.2467 & 0.1893 \\ 
1.3440 & 0.1487 & -0.6174 & 0.8890 & -0.3282 & 1.4662 \\ 
0.1814 & -0.4761 & -0.0402 & 0.7326 & 0.7654 & -0.1000 \\ 
-8.8974 & 0.6786 & 8.5682 & -0.2443 & -8.4883 & 1.3996 
\end{bmatrix}$$

## Step 4: Calculate Keys, Queries and Value Matrix   

- $\mathbf{X} \times \mathbf{W_q}$
- $\mathbf{X} \times \mathbf{W_k}$
- $\mathbf{X} \times \mathbf{W_v}$

$$\mathbf{X} \times \mathbf{W_q} = \begin{bmatrix} 
4.2932 & -2.4849 & 2.4194 & 4.0865 & 1.5162 & -0.6586 \\ 
-0.6586 & -2.4849 & 2.4194 & 4.0865 & 4.2932 & -2.4849 \\ 
1.4587 & 0.4809 & 1.7221 & 0.7082 & -0.1234 & 0.7995 
\end{bmatrix}$$

$$\mathbf{X} \times \mathbf{W_k} = \begin{bmatrix} 
3.5049 & -0.2332 & -13.0061 & 4.3009 & 7.2783 & -1.5149 \\ 
-1.5149 & -0.2332 & -13.0061 & 4.3009 & 3.5049 & -0.2332 \\ 
-0.3752 & -0.4537 & 1.1995 & 0.1121 & 1.2829 & -0.0216 
\end{bmatrix}$$


$$\mathbf{X} \times \mathbf{W_v} = \begin{bmatrix} 
-3.7603 & 1.7251 & 15.5114 & -1.3864 & -5.8559 & 5.7465 \\ 
5.7465 & 1.7251 & 15.5114 & -1.3864 & -3.7603 & 1.7251 \\ 
-0.5320 & 0.2449 & 1.8943 & -0.0822 & -0.9636 & 0.2034 
\end{bmatrix}$$


![Dimension Diagram](assets/17.%20Multihead%20Attention/dim.png)


- Each row corresponds to one token
- Each token is 6 dimension.

## Step 5: Unroll last dimension of Keys, Queries and Values to include $num_{heads}$ and $head_{dim}$

$head\_dimension = \frac{d_{out}}{n_{head}} = \frac{6}{2} =3$

![Change Diagram](assets/17.%20Multihead%20Attention/change.png)

$$\mathbf{Q_{reshaped}} = \begin{bmatrix} 
\begin{bmatrix} -0.4888 & 0.2361 & 2.8463 \\ -0.2184 & 5.4503 & -1.8915 \end{bmatrix} \\
\begin{bmatrix} -2.3531 & -0.7912 & 2.0534 \\ -4.3369 & 3.2125 & 1.2578 \end{bmatrix} \\
\begin{bmatrix} -2.5745 & 0.2893 & 1.1454 \\ 0.9021 & 1.5632 & 0.6930 \end{bmatrix} 
\end{bmatrix}$$

$$\mathbf{K_{reshaped}} = \begin{bmatrix} 
\begin{bmatrix} 0.4143 & -1.4023 & -2.7131 \\ 3.4907 & -2.1993 & 0.2381 \end{bmatrix} \\
\begin{bmatrix} 1.1957 & 1.3712 & 0.6885 \\ -1.5484 & 4.2152 & 2.1248 \end{bmatrix} \\
\begin{bmatrix} -0.1226 & 0.1155 & 0.4755 \\ -0.0176 & 0.8339 & 0.7582 \end{bmatrix} 
\end{bmatrix}$$

$$\mathbf{V_{reshaped}} = \begin{bmatrix} 
\begin{bmatrix} -3.6194 & 2.0935 & 1.3879 \\ 2.1231 & -1.2262 & -0.2556 \end{bmatrix} \\    
\begin{bmatrix} 1.1106 & -0.4063 & -0.5588 \\ 1.8222 & 1.8721 & 0.4929 \end{bmatrix} \\    
\begin{bmatrix} -1.6594 & 0.1052 & -0.0468 \\ 0.8916 & -1.4384 & -0.5651 \end{bmatrix} 
\end{bmatrix}$$

Each matrix inside matrix corresponds to one token and which are divided into 2 head.

Practically: 2 People paying attention to each token

## Step 6: Group matrices by number of head   
Why: For computation by each head

We will get attention score matrix for each head saperately then we will concatinate

![Flip Diagram](assets/17.%20Multihead%20Attention/flip.png)

New dimension: $(1,2,3,3)$

$$\mathbf{Q_{transformed}} = \begin{bmatrix} 
\begin{bmatrix} -0.4888 & 0.2361 & 2.8463 \\ -2.3531 & -0.7912 & 2.0534 \\-2.5745 & 0.2893 & 1.1454  \end{bmatrix} \\
\begin{bmatrix} -0.2184 & 5.4503 & -1.8915 \\ -4.3369 & 3.2125 & 1.2578 \\ 0.9021 & 1.5632 & 0.6930 \end{bmatrix} 
\end{bmatrix}$$

$$\mathbf{K_{transformed}} = \begin{bmatrix} 
\begin{bmatrix} 0.4143 & -1.4023 & -2.7131\\1.1957 & 1.3712 & 0.6885 \\-0.1226 & 0.1155 & 0.4755 \end{bmatrix} \\
\begin{bmatrix}  3.4907 & -2.1993 & 0.2381 \\ -1.5484 & 4.2152 & 2.1248 \\ -0.0176 & 0.8339 & 0.7582 \end{bmatrix}\end{bmatrix}$$

$$\mathbf{V_{transformed}} = \begin{bmatrix} 
\begin{bmatrix} -3.6194 & 2.0935 & 1.3879 \\ 1.1106 & -0.4063 & -0.5588\\-1.6594 & 0.1052 & -0.0468 \end{bmatrix} \\    
\begin{bmatrix} 2.1231 & -1.2262 & -0.2556  \\ 1.8222 & 1.8721 & 0.4929\\ 0.8916 & -1.4384 & -0.5651 \end{bmatrix}
\end{bmatrix}$$

- Each inner matrix for each head computation and 3 rows inside it represent 3 token for each head

## Step 7: Find Attention Score
$\mathbf{Attention\_Score} = \mathbf{Q} \times \mathbf{K}^T$

$$\mathbf{K_{transformed}}^\top = \begin{bmatrix}
\begin{bmatrix}
0.4143 & 1.1957 & -0.1226 \\
-1.4023 & 1.3712 & 0.1155 \\
-2.7131 & 0.6885 & 0.4755\\
\end{bmatrix} \\

\begin{bmatrix}
3.4907 & -1.5484 & -0.0176 \\
-2.1993 & 4.2152 & 0.8339 \\
0.2381 & 2.1248 & 0.7582
\end{bmatrix}
\end{bmatrix}$$


$$
\mathbf{Q_{transformed} \times K_{transformed}^T} =
\begin{bmatrix}
\begin{bmatrix} -0.6347 & 0.0932 & -1.0212 \\ -1.4575 & 3.5434 & 6.9408 \\ -1.0186 & 3.9301 & 7.6656 \end{bmatrix} \\
\begin{bmatrix} -8.9021 & 23.1722 & 3.7513 \\ -15.5508 & 28.7933 & 5.6098 \\ 2.2056 & 5.6847 & 2.5589 \end{bmatrix}
\end{bmatrix}
$$
  

Dimension: $(b, \text{num}_{\text{head}}, \text{num}_{\text{tokens}}, \text{num}_{\text{tokens}})$

We have matrix showing how much attention score related to one word with another

## Step 8: Find Attention weights
Mask attention scores to implement causal attention

$$
\mathbf{(Q_{transformed} \times K_{transformed}^T)_{masked}} =
\begin{bmatrix}
\begin{bmatrix}
-0.6347 & -\infty & -\infty \\
-1.4575 & 3.5434 & -\infty \\
-1.0186 & 3.9301 & 7.6656
\end{bmatrix} \\
\begin{bmatrix}
-8.9021 & -\infty & -\infty \\
-15.5508 & 28.7933 & -\infty \\
2.2056 & 5.6847 & 2.5589
\end{bmatrix}
\end{bmatrix}
$$

- If we implemet softmax $-\infty$ will become 0 and all row sum up to 1

- Before that we need to divide by $\sqrt {head_{dim}}$ = $\sqrt{\frac {6}{2}}$ =$\sqrt 3$

$$
\mathbf{\frac{(Q_{transformed} \times K_{transformed}^T)_{masked}}{\sqrt{3}}} =
\begin{bmatrix}
\begin{bmatrix}
-0.3664 & -\infty & -\infty \\
-0.8412 & 2.0459 & -\infty \\
-0.5880 & 2.2686 & 4.4256
\end{bmatrix} \\
\begin{bmatrix}
-5.1388 & -\infty & -\infty \\
-8.9754 & 16.6212 & -\infty \\
1.2732 & 3.2819 & 1.4770
\end{bmatrix}
\end{bmatrix}
$$

- After applying softmax
$$
\mathbf{softmax\left(\frac{(Q_{transformed} \times K_{transformed}^T)_{masked}}{\sqrt{3}}\right)} =
\begin{bmatrix}
\begin{bmatrix}
1.0000 & 0.0000 & 0.0000 \\
0.0724 & 0.9276 & 0.0000 \\
0.0015 & 0.0878 & 0.9107
\end{bmatrix} \\
\begin{bmatrix}
1.0000 & 0.0000 & 0.0000 \\
0.0000 & 1.0000 & 0.0000 \\
0.0807 & 0.7843 & 0.1350
\end{bmatrix}
\end{bmatrix}
$$

- We can also implement dropout after this 

## Step 9: Compute Context Vector

$$
\mathbf{Context\ Vector} = \mathbf{Attention\ Weights} \times \mathbf{Values}
$$

$$
\mathbf{Context\ Vector} =
\begin{bmatrix}
\begin{bmatrix}
1.0000 & 0.0000 & 0.0000 \\
0.0724 & 0.9276 & 0.0000 \\
0.0015 & 0.0878 & 0.9107
\end{bmatrix} \\
\begin{bmatrix}
1.0000 & 0.0000 & 0.0000 \\
0.0000 & 1.0000 & 0.0000 \\
0.0807 & 0.7843 & 0.1350
\end{bmatrix}
\end{bmatrix} 
\times 
\begin{bmatrix} 
\begin{bmatrix} 
-3.6194 & 2.0935 & 1.3879 \\ 
1.1106 & -0.4063 & -0.5588 \\ 
-1.6594 & 0.1052 & -0.0468 
\end{bmatrix} \\  
\begin{bmatrix} 
2.1231 & -1.2262 & -0.2556  \\ 
1.8222 & 1.8721 & 0.4929 \\ 
0.8916 & -1.4384 & -0.5651 
\end{bmatrix}
\end{bmatrix}
$$



$$
\mathbf{Context\ Vector} =
\begin{bmatrix}
\begin{bmatrix}
-3.6194 & 2.0935 & 1.3879 \\
0.8036 & -0.2926 & -0.4385 \\
-1.5123 & 0.0901 & -0.0603
\end{bmatrix} \\
\begin{bmatrix}
2.1231 & -1.2262 & -0.2556 \\
1.8222 & 1.8721 & 0.4929 \\
1.7251 & 1.0674 & 0.3078
\end{bmatrix}
\end{bmatrix}
$$

Dimension:  
$(b, \text{num}_{\text{head}}, \text{num}_{\text{tokens}}, \text{num}_{\text{tokens}}) 
\times 
(b, \text{num}_{\text{heads}}, \text{num}_{\text{tokens}}, \text{head}_{\text{dim}}) 
\rightarrow 
(b, \text{num}_{\text{heads}}, \text{num}_{\text{tokens}}, \text{head}_{\text{dim}})$

Each row represent context vector for particular token 

- We need to merge $num_{head}$ and $head_{dim}$ because resultant context vector matrix should have dimension of $d_{out}$

## Step 10 : Reformat context Vector
![Flip2 Diagram](assets/17.%20Multihead%20Attention/flip2.png)

Making closer to merge easily

$$
\mathbf{Context\ Vector^\top } =
\begin{bmatrix}
\begin{bmatrix}
-3.6194 & 2.0935 & 1.3879 \\
2.1231 & -1.2262 & -0.2556 \\
\end{bmatrix} \\

\begin{bmatrix}
0.8036 & -0.2926 & -0.4385 \\
1.8222 & 1.8721 & 0.4929 \\
\end{bmatrix} \\

\begin{bmatrix}
-1.5123 & 0.0901 & -0.0603 \\
1.7251 & 1.0674 & 0.3078
\end{bmatrix}
\end{bmatrix}
$$

Each inner matrix represent each token

## Step 11: Combine or Flatten each token


$$
\mathbf{Context\ Vector^\top } =
\begin{bmatrix}
\begin{bmatrix}
-3.6194 & 2.0935 & 1.3879 &
2.1231 & -1.2262 & -0.2556 \\
\end{bmatrix} \\

\begin{bmatrix}
0.8036 & -0.2926 & -0.4385 &
1.8222 & 1.8721 & 0.4929 \\
\end{bmatrix} \\

\begin{bmatrix}
-1.5123 & 0.0901 & -0.0603 &
1.7251 & 1.0674 & 0.3078
\end{bmatrix}
\end{bmatrix}
$$

- Rows represent context vector for each token

Dimension : $(b,num_{token}, d_{out})$ = (1, 3, 6)