# Token Embedding
## Why token embeddings are needed?

- Computer need numerical representation of word
- We used Token IDs
- We cannot use directly Token IDs , cat and kitten are similar but `token id doesn't capture sementic meaning`

### Why not One hot encoding
"dog" --> [0,0,1,0,0,0,0,...]  
"puppy" --> [0,0,0,0,1,0,0,...]  

Again random number assignment

### Let's encode every wor a vector
- What is dimension
> On the basis of feature...

<img src="assets/10. Embedding/vector.png" width="500" />   

- Can see which is near to each other and farther from each other.

> From here vector embedding comes from. 

## How do we come up with those values of features?
- We can train Neural Network to Construct vector Embedding

<img src="assets/10. Embedding/nn.png" width="500" />   

## How are token embeddings created for LLMs?
1. Create Vocabulary- We have token and token id into it.
2. Every token id converted into vector.

> What was vector Embedding dimension for training GPT2 and what was vocabulary size.  

> `Vector Embedding dimension`: 768 and 1600 for larger. 
> `Vocabulary size`: 50,257

Weights: 768 * 50,257 [Embedding layer Weight matrix]

<img src="assets/10. Embedding/size.png" width="500" />   


Steps: 
- Initialise embedding weights woth random values
[ This is starting point for LLM learning process]

- These weights are `optimised as part of LLM training process`.
> Backpropagation were applied 

## LLM Training
- Weight of Embedding are optimised
- Weights of next word prediction are also optimised

## Lookup operation
Embedding layer is a lookup operation that retrives rows from the embedding layer weight matrix using a token ID. 

<img src="assets/10. Embedding/summary.png" width="500" />   
