# Two Stages: Pretraining + Finetuning

## 1. Pretraining
Meaning: `Training` on large dataset  
Let (1 Token=1 Word)  
Locations where data collected for training GPT3.      
[SOURCE](https://arxiv.org/pdf/2005.14165)  
<img src="assets/3. Stages/data.png" width="500" />    
+ [CommonCRAWL](https://commoncrawl.org/)
+ [Wikipeida](https://www.wikipedia.org/)
+ [WebText2](https://openwebtext2.readthedocs.io/en/latest/#download-raw-scrapes-version)
+ Books

Initially it was trained for word completion

<img src="assets/3. Stages/notice.png" width="500" />     

[SOURCE](https://openai.com/index/language-unsupervised/)  
> Note: Pretrainined model also known as `Foundational Model` and

## 2. Finetuning
Meaning: `Refinement` by training on narrower dataset, specific to a particular task or domain.  
<img src="assets/3. Stages/fine-hervy.png" width="500" />  

[SOURCE >>](https://openai.com/index/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program/)
[CHECK](https://www.harvey.ai/)

---
Now I know source where they trained I will also train and create `FOUNDATIONAL MODEL`ðŸ¥³  
But  
<img src="assets/3. Stages/cost.png" width="500" />   

<img src="assets/3. Stages/nomoney.gif" width="100" />   

---  
Steps of building LLM:  
1. Training on large corpus of text data (RAW text: regular text witout labelling info)  
2. First Training Stage: `Pre-Training`  
    Creating initial pretrained LLM, base/foundational model.
3. Further train on labelled data: `Fine Tuning`    
    A. Instruction Finetuning `Dataset consist of Instruction - Answer`  
    B. Finetuning for classfication task `Dataset consist of Text - Associated Label`
