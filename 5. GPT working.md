## 1. Progression from Transformer to GPT
2017 Attention is all you need
+ Introduing self attention Mechnaism
+ Introduce Transformer

2018 Generative Pre-Training
+ Unsupervised Pre-Training

2019 Language Model are Unsupervised Multitask Learners
+ 4 models used (GPT 2 architecture)
+ GPT-2 SMALL, GPT-2 MEDIUM, GPT-2 LARGE, GPT-2 EXTRA LARGE

2020 Langauge Model are Few Shot Learning
+ 8 models used (GPT 3 architecture)
+ Can do numerous things

2022 GPT 3.5 (Commercially Viral)

2024 GPT 4

> What is Difference between GPT and Transformer :       
> GPT borrow architecture from Transformer and don't have Encoder block  

<img src="assets/5. Working/borrow.gif" width="200" />    

---  
## 2. Zero Shot Vv Few Shot Learning

`Zero Shot Learning` : Model predict answer given any description
> "Hey Convert this into French"

`One Shot Learning` : Model sees single example of task then perform
> "Here is translation of XYZ, Translate PQR into French"

`Few Shot Learning` : Model sees few examples of task then perform  
+ GPT 3 was few shot learner, although it was only trained for language translation.

> GPT 4 is/was few shot learner, but can even do zero shot learning.

###### Zero shot learning is getting result without providing example, Few shot learning is getting result by providing some example.

---
`ALREADY COVERED`

`Token` : Complex procedure to convert work into token called `Tokenization` 
OR
Unit or word which models read

+ GPT3 trained on 300B token, Pretraining costs $4.6 Million.
+ Pre trained models are BASE/FOUNDATIONAL models which can be used for further finetuning.
+ Many pretrained LLMs are available as Open source models.
+ GPT consist of only DECODER block
---
## 3. Why GPT known as Auto Regressor model and why falls under Unsupervised Learning

> GPT model are simple tranined on Next word predictor

<img src="assets/5. Working/gpt-work.png" width="500" />    

`Unsupervised`: We use structure of data to create label itself

`Auto Regressive`:Previous output used as Input for future prediction

---
+ GPT architecture is simpler, have no encoder block 

Original Transformer: 6 encoder-decoder block
GPT 3: 96 transformer layers, 175 billion parameters


---
## 4. Emergent Behaviour  
> Although GPT only tranined for translation task, it perform other tasks too.