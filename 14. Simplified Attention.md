## Context Vector
How much each word is related to other words

`Aim`: Convert Embedding Vector to Context Vector

<img src="assets/14. SimAttention/sim.png" width="500" />    

x1: Token embedding of 1
z1: Context vector of 1

`Query`: word for which we are finding context vector

## Step 1:
(Compute Attention Scores)
Calculate attention attention score between each token for specific query vector

> Which mathematical operation require to find importance between 2 vector?

>Dot product between 2 vectors  
A⋅B= ∣A∣.∣B∣ cosθ 

<img src="assets/14. SimAttention/dotprod.gif" width="500" />    


## Step 2:
(Compute Attention Weights)
Perform Normalization: Know % of each importance

> Attention Score vs Attention Weight  
> Both represent same things, `Attention Weights` sum up to 1.

There are 2 type of normalization:
`Simply divide by sum`
`Use softmax`: smaller value neglected (preferable)

**Pytorch Softmax Implementation**  
Why -max in numerator? [INTERVIEW QUESTION] 
```python
def own_softmax(self, x)

    maxes = torch.max(x, 1, keepdim=True)[0]
    x_exp = torch.exp(x-maxes)
    x_exp_sum = torch.sum(x_exp, 1, keepdim=True)

    return x_exp/x_exp_sum
```

## Step 3:
(Compute Context Vectors)
`Scaling`
Multiply input vector with each corresponding attention weight (normalization %)

Find vector summation

<img src="assets/14. SimAttention/final.png" width="500" />    
