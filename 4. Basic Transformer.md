## 1. Transformer Architecture 
1. Most modern LLMS rely in the transformer architecture.  
Paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762)
- initially this paper published for Translation Task (Englist to German and French)  

<img src="assets/4. LLM Basic/word.png" width="500" />    

"This is an Example" -> Broken into Token -> `Encoder` (Capture sementic meaning) | Some Neural Network are also trained for this step  -> Vector Embedding

"Das est ein" -> Input text-> Preprocessing -> `Decoder` -> Output Layer -> "Das est ein `Beispiel`"

+ `Encoder`: Encode input text into Vector
+ `Decoder`: Generate output text from encodeed vectors

> Note:In GPT there is no Encoder

##  2. Attention
#### Key part of Transformer
<img src="assets/4. LLM Basic/attention.png" width="500" />    

+ Allow model to weigh importance of different words/ tokens relative to each other.

+ Enable model to capture long range dependencies

## 3. Later Variation of Transformer
BERT: Bidirectional encoder representations from Transformers.  
+ Receive input where words are rendomly masked during training  

This is an ____ of how LLM _____ perform
> This is an `example` of how LLM `can` perform.

GPT: Generative Pretrained Transformers.
+ Learn to generate one word at a time  

This is an example of how LLM can perform.
> This is an example of how LLM `can` perform.

## 4. Transformer VS LLM
`Not all transformers are LLMs`  
+ Transformers can also be used for Computer Vision. [Vision Transformer](https://viso.ai/deep-learning/vision-transformer-vit/)

`Not all LLMs are Transformer`
+ LLMs can be based on RNN or CNN architecture as well