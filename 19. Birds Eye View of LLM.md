## Birds Eye View

Now we will fo through LLM Architecture

### Coding LLM Architecture

<img src="assets/19. Birds Eye View/view.png" width="500" />   

- Expanding Transformer Block

<img src="assets/19. Birds Eye View/trans.png" width="500" />   

Seen So far:  
a. Input Tokenization  
b. Embedding (Token + Positional)  
c. Masked Multi-Head Attention  

<img src="assets/19. Birds Eye View/sofar.png" width="500" />   

Will Look:
a. Transformer Block

- Upto the size of GPT 2 :-  
Parms: 117M   
Layer : 12    
$d_{model}$ : 768 

- OpenAI has made GPT 2 weights public, GPT 3 or 4 weights have not been yet made public.

- Configuration:

```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,    # Vocabulary size
    "context_length": 1024, # Context length
    "emb_dim": 768,         # Embedding dimension
    "n_heads": 12,          # Number of attention heads
    "n_layers": 12,         # Number of layers
    "drop_rate": 0.1,       # Dropout rate
    "qkv_bias": False       # Query-Key-Value bias
}
```