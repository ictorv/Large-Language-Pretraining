## Multihead Attention

1. The term "multi-head" refer to dividing the attention mechanism into multiole heads

    **A.** Stacking multiple single head attention layers.
  
    **B.** Implementing multi-head atention involves creating multiple instance of the self-attetnion mechanism; each with it's own weights and then combining their outputs.  

2. This can be computationally intensive, but it makes LLMs powerful at comples pattern recognition tasks.

     
<img src="assets/17. Multihead Attention/multi.png" width="500" />   


### Output
<img src="assets/17. Multihead Attention/out.png" width="500" />   
